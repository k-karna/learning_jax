{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``JAX.Array`` is default array implementation in JAX, but we usually create arrays via JAX API functions. ``jax.numpy`` provides almost all familiar array construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax \n",
    "import jax.numpy as jnp \n",
    "\n",
    "x = jnp.arange(5)\n",
    "isinstance(x, jax.Array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{CpuDevice(id=0)}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to inspect device\n",
    "x.devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An array may be __sharded__ across multiple device in parallel programming, which can be inspected by ``sharding``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SingleDeviceSharding(device=CpuDevice(id=0))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.sharding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Transformations__ accept a function as an argument, and return a new transformed function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5749999\n"
     ]
    }
   ],
   "source": [
    "def selu(x, alpha=1.67, lambda_ = 1.05):\n",
    "    return lambda_ * jnp.where(x >0, x, alpha * jnp.exp(x) - alpha)\n",
    "\n",
    "selu_jit = jax.jit(selu)\n",
    "print(selu_jit(1.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Another way of JIT compilation\n",
    "\n",
    "@jax.jit\n",
    "def selu(x, alpha=1.67, lambda_ = 1.05):\n",
    "    return lambda_ * jnp.where(x >0, x, alpha * jnp.exp(x) - alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Tracers__ are used as standin for JAX array to determine the sequence of operations performed by a python function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traced<ShapedArray(int32[5])>with<DynamicJaxprTrace(level=1/0)>\n"
     ]
    }
   ],
   "source": [
    "@jax.jit \n",
    "def f(x):\n",
    "    print(x)\n",
    "    return x+1\n",
    "\n",
    "x = jnp.arange(5)\n",
    "result = f(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__JAXPR__ is an intermediate representation of a computation that is generated by JAX, and is forwarded to XLA for compilation and execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selu(x, alpha=1.67, lambda_ = 1.05):\n",
    "    return lambda_ * jnp.where(x >0, x, alpha * jnp.exp(x) - alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ \u001b[34m\u001b[22m\u001b[1mlambda \u001b[39m\u001b[22m\u001b[22m; a\u001b[35m:i32[5]\u001b[39m. \u001b[34m\u001b[22m\u001b[1mlet\n",
       "    \u001b[39m\u001b[22m\u001b[22mb\u001b[35m:bool[5]\u001b[39m = gt a 0\n",
       "    c\u001b[35m:f32[5]\u001b[39m = convert_element_type[new_dtype=float32 weak_type=False] a\n",
       "    d\u001b[35m:f32[5]\u001b[39m = exp c\n",
       "    e\u001b[35m:f32[5]\u001b[39m = mul 1.6699999570846558 d\n",
       "    f\u001b[35m:f32[5]\u001b[39m = sub e 1.6699999570846558\n",
       "    g\u001b[35m:f32[5]\u001b[39m = pjit[\n",
       "      name=_where\n",
       "      jaxpr={ \u001b[34m\u001b[22m\u001b[1mlambda \u001b[39m\u001b[22m\u001b[22m; h\u001b[35m:bool[5]\u001b[39m i\u001b[35m:i32[5]\u001b[39m j\u001b[35m:f32[5]\u001b[39m. \u001b[34m\u001b[22m\u001b[1mlet\n",
       "          \u001b[39m\u001b[22m\u001b[22mk\u001b[35m:f32[5]\u001b[39m = convert_element_type[new_dtype=float32 weak_type=False] i\n",
       "          l\u001b[35m:f32[5]\u001b[39m = select_n h j k\n",
       "        \u001b[34m\u001b[22m\u001b[1min \u001b[39m\u001b[22m\u001b[22m(l,) }\n",
       "    ] b a f\n",
       "    m\u001b[35m:f32[5]\u001b[39m = mul 1.0499999523162842 g\n",
       "  \u001b[34m\u001b[22m\u001b[1min \u001b[39m\u001b[22m\u001b[22m(m,) }"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = jnp.arange(5)\n",
    "jax.make_jaxpr(selu)(x) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JAX is designed to work with __pure__ functions. Pure functions are those that always produces the same output for the same input or determinisitc, and has no side-effects.\n",
    "\n",
    "Side-effect within a function occurs when a function:\n",
    "- modifies a variable outside its local scope\n",
    "- modifies a mutable object passed as an argument\n",
    "- performs I/O operations (printing/over-writing, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printed x: Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace(level=1/0)>\n",
      "{ \u001b[34m\u001b[22m\u001b[1mlambda \u001b[39m\u001b[22m\u001b[22m; a\u001b[35m:f32[]\u001b[39m. \u001b[34m\u001b[22m\u001b[1mlet\n",
      "    \u001b[39m\u001b[22m\u001b[22mb\u001b[35m:f32[]\u001b[39m = log a\n",
      "    c\u001b[35m:f32[]\u001b[39m = log 2.0\n",
      "    d\u001b[35m:f32[]\u001b[39m = div b c\n",
      "  \u001b[34m\u001b[22m\u001b[1min \u001b[39m\u001b[22m\u001b[22m(d,) }\n"
     ]
    }
   ],
   "source": [
    "def log2_with_print(x):\n",
    "\n",
    "    print(\"printed x:\", x)\n",
    "    ln_x = jnp.log(x)\n",
    "    return ln_x / jnp.log(2.0)\n",
    "\n",
    "print(jax.make_jaxpr(log2_with_print)(3.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JIT Compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.19 ms ± 179 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "def selu(x, alpha=1.67, lambda_ = 1.05):\n",
    "    return lambda_ * jnp.where(x >0, x, alpha * jnp.exp(x) - alpha) \n",
    "\n",
    "x = jnp.arange(1000000)\n",
    "%timeit selu(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "759 µs ± 13.5 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "#JIT compilation\n",
    "selu_jit = jax.jit(selu)\n",
    "%timeit selu_jit(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot JIT everywhere. Some cases are when functions are:\n",
    "- having control flow dependent on runtime value\n",
    "- not of static state, but dynamic\n",
    "- having non-JAX operations within itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def f(x):\n",
    "#   if x > 0:  # Depends on runtime value of x\n",
    "#     return x\n",
    "#   else:\n",
    "#     return 2 * x\n",
    "# jax.jit(f)(10)  # Raises TracerBoolConversionError\n",
    "\n",
    "\n",
    "# def g(x, n):\n",
    "#   i = 0\n",
    "#   while i < n:\n",
    "#     i += 1\n",
    "#   return x + i\n",
    "\n",
    "# jax.jit(g)(10, 20)  \n",
    "\n",
    "\n",
    "# def f(x):\n",
    "#   return jnp.arange(x)  # Shape of output depends on runtime value of x\n",
    "# jax.jit(f)(10)  # Raises ConcretizationTypeError\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "# def f(x):\n",
    "#   return np.sin(x)  # Non-JAX operation\n",
    "# jax.jit(f)(10)  # Raises TracerArrayConversionError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Special __control flow operators__ can be used, or we can JIT-compile only part of the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(30, dtype=int32, weak_type=True)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#While look conditioned on x and y with a jitted body \n",
    "\n",
    "@jax.jit\n",
    "def loop_body(prev_i):\n",
    "    return prev_i + 1\n",
    "\n",
    "def g_inner_jitted(x, n):\n",
    "    i = 0\n",
    "    while i < n:\n",
    "        i = loop_body(i)\n",
    "    return x + i\n",
    "g_inner_jitted(10, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way is to __mark argument as static__ with specifying ``static_argnums`` or ``static_argnames``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "f_jit_correct = jax.jit(f, static_argnums=0)\n",
    "print(f_jit_correct(10))\n",
    "\n",
    "g_jitted_correct = jax.jit(g, static_argnames=['n'])\n",
    "print(g_jitted_correct(10, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To specify such argument when using __jit__ as decorator, a common pattern is to use ``functools.partial()``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "@partial(jax.jit, static_argnames=['n'])\n",
    "def g_jit_decorated(x, n):\n",
    "    i = 0\n",
    "    while i < n:\n",
    "        i += 1\n",
    "    return x + i\n",
    "print(g_jit_decorated(10, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have jitted ``f = jax.jit(g)``, then in JAX subsequent calls of ``f`` will reuse the cached code, unless we specify ``static_argnums`` then cached code will only be used for specified static values. For any other value, a new compilation will occur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JIT called in a loop with partials:\n",
      "192 ms ± 8.86 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "JIT called in a loop with lambdas:\n",
      "190 ms ± 2.09 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "JIT called in a loop with normal:\n",
      "1.22 ms ± 23 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "def unjitted_loop_body(prev_i):\n",
    "    return prev_i + 1\n",
    "\n",
    "def g_inner_jitted_partial(x, n):\n",
    "    i = 0\n",
    "    while i < n:\n",
    "        #every time partial returns a function with different hash\n",
    "        i = jax.jit(partial(unjitted_loop_body))(i)\n",
    "    return x + i\n",
    "\n",
    "def g_inner_jitted_lambda(x, n):\n",
    "    i = 0\n",
    "    while i < n:\n",
    "        #every time lambda returns a function with different hash\n",
    "        i = jax.jit(lambda x: unjitted_loop_body(x))(i)\n",
    "    return x + i\n",
    "\n",
    "def g_inner_jitted_normal(x, n):\n",
    "    i = 0\n",
    "    while i < n:\n",
    "        #JAX can find cached version of the function\n",
    "        i = jax.jit(unjitted_loop_body)(i)\n",
    "    return x + i\n",
    "\n",
    "print(\"JIT called in a loop with partials:\")\n",
    "%timeit g_inner_jitted_partial(10, 20).block_until_ready()\n",
    "\n",
    "print(\"JIT called in a loop with lambdas:\")\n",
    "%timeit g_inner_jitted_lambda(10, 20).block_until_ready()   \n",
    "\n",
    "print(\"JIT called in a loop with normal:\")\n",
    "%timeit g_inner_jitted_normal(10, 20).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
