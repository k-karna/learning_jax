{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import math \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "import matplotlib\n",
    "from IPython.display import set_matplotlib_formats \n",
    "from matplotlib.colors import to_rgba\n",
    "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
    "import seaborn as sns\n",
    "sns.set_context('talk')\n",
    "sns.set_style('whitegrid')  \n",
    "\n",
    "import time \n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JAX Framework API for dealing with data array is ``jax.numpy``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jax version:  0.4.30\n"
     ]
    }
   ],
   "source": [
    "import jax \n",
    "import jax.numpy as jnp \n",
    "print(\"jax version: \", jax.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#Creating an array of zeros with shape [2,5] \n",
    "\n",
    "a = jnp.zeros((2,5), dtype=jnp.float32)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "#creating an array with values of 0 to 5 using jnp.arange\n",
    "b = jnp.arange(6)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "jaxlib.xla_extension.ArrayImpl"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.__class__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JAX can execute the same code on different backends - CPU, GPU, TPU. ``ArrayImpl`` represents an array which is on one of the backends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{CpuDevice(id=0)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#to change the device of the array, we can get \n",
    "\n",
    "b_cpu = jax.device_get(b)\n",
    "print(b_cpu.__class__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device Put:<class 'jaxlib.xla_extension.ArrayImpl'> on {CpuDevice(id=0)}\n"
     ]
    }
   ],
   "source": [
    "#To explicitly push a numpy array to the GPU, \n",
    "# we can use ``jax.device_put``\n",
    "\n",
    "b_gpu = jax.device_put(b_cpu)\n",
    "print(f\"Device Put:{b_gpu.__class__} on {b_gpu.devices()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JAX can handle any device class when you try to perform a numpy operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([ 0,  2,  4,  6,  8, 10], dtype=int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_cpu + b_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, if we call ``out = jnp.matmul(b, b)``, JAX first returns a placeholder array for out which may not be filled with the values as soon as the function calls finishes. This way, Python will not block the execution of follow-up statements, but instead only does it whenever we strictly need the value of ``out``, for instance for printing or putting it on CPU. PyTorch uses a very similar principle to allow asynchronous computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Immutable Tensors \n",
    "\n",
    "``In-place`` operations are not allowed like ``b[0]=1``. JAX requires programs to be \"pure\" functions.\n",
    "\n",
    "So, we can use ``b.at[0].set(1)`` and its returns a new array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original array:  [0 1 2 3 4 5]\n",
      "new array:  [1 1 2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "b_new = b.at[0].set(1)\n",
    "print(\"original array: \", b)\n",
    "print(\"new array: \", b_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Psuedo Random Numbers in JAX \n",
    "\n",
    "``np.random.normal()`` will 5 different numbers at 5 separate call, as every execution changes the state/seed of the pseudo random number generator (PRNG)\n",
    "\n",
    "JAX solves it by explicitly passing and iterating PRNG state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX - Random Number 1:  -0.18471177\n",
      "JAX - Random Number 2:  -0.18471177\n",
      "Numpy - Random Number 1:  0.4967141530112327\n",
      "Numpy - Random Number 2:  -0.13826430117118466\n"
     ]
    }
   ],
   "source": [
    "# A non-desirable way of generating psuedo random numbers\n",
    "\n",
    "jax_random_number_1 = jax.random.normal(rng)\n",
    "jax_random_number_2 = jax.random.normal(rng) \n",
    "print(\"JAX - Random Number 1: \", jax_random_number_1)\n",
    "print(\"JAX - Random Number 2: \", jax_random_number_2)\n",
    "\n",
    "#Typical random numbers in Numpy \n",
    "np.random.seed(42)\n",
    "np_random_number_1 = np.random.normal()\n",
    "np_random_number_2 = np.random.normal()\n",
    "print(\"Numpy - Random Number 1: \", np_random_number_1)\n",
    "print(\"Numpy - Random Number 2: \", np_random_number_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get different random number every time we sample, we can _split_ the PRNG state to get usable subkeys every time we need a pseudo-random number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX - Random Number 1:  0.107961535\n",
      "JAX - Random Number 2:  -1.2226542\n"
     ]
    }
   ],
   "source": [
    "rng, subkey1, subkey2 = jax.random.split(rng, num=3) \n",
    "jax_random_number_1 = jax.random.normal(subkey1)\n",
    "jax_random_number_2 = jax.random.normal(subkey2)\n",
    "print(\"JAX - Random Number 1: \", jax_random_number_1)\n",
    "print(\"JAX - Random Number 2: \", jax_random_number_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FUNCTIONs with JAXPR\n",
    "\n",
    "We can check which operations are performed on which array, and what shapes the arrays are. \n",
    "\n",
    "consider function: \n",
    "\n",
    "$$y = \\frac{1}{\\lvert x \\rvert} \\sum_{i} [(x_{i}+2)^2 + 3]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input [0. 1. 2.]\n",
      "Output 12.666667\n"
     ]
    }
   ],
   "source": [
    "def simple_graph(x):\n",
    "    x = x+2\n",
    "    x = x ** 2\n",
    "    x = x + 3\n",
    "    y = x.mean()\n",
    "    return y \n",
    "\n",
    "inp = jnp.arange(3, dtype=jnp.float32)\n",
    "print(\"Input\", inp)\n",
    "print(\"Output\", simple_graph(inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ \u001b[34m\u001b[22m\u001b[1mlambda \u001b[39m\u001b[22m\u001b[22m; a\u001b[35m:f32[3]\u001b[39m. \u001b[34m\u001b[22m\u001b[1mlet\n",
       "    \u001b[39m\u001b[22m\u001b[22mb\u001b[35m:f32[3]\u001b[39m = add a 2.0\n",
       "    c\u001b[35m:f32[3]\u001b[39m = integer_pow[y=2] b\n",
       "    d\u001b[35m:f32[3]\u001b[39m = add c 3.0\n",
       "    e\u001b[35m:f32[]\u001b[39m = reduce_sum[axes=(0,)] d\n",
       "    f\u001b[35m:f32[]\u001b[39m = div e 3.0\n",
       "  \u001b[34m\u001b[22m\u001b[1min \u001b[39m\u001b[22m\u001b[22m(f,) }"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To view JAXPR representation, we can use `jax.make_jaxpr`. \n",
    "#Since, tracing depends on the shape of the input,\n",
    "#we need to pass an input.\n",
    "\n",
    "jax.make_jaxpr(simple_graph)(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ \u001b[34m\u001b[22m\u001b[1mlambda \u001b[39m\u001b[22m\u001b[22m; a\u001b[35m:f32[3]\u001b[39m. \u001b[34m\u001b[22m\u001b[1mlet\n",
       "    \u001b[39m\u001b[22m\u001b[22mb\u001b[35m:f32[3]\u001b[39m = integer_pow[y=2] a\n",
       "    c\u001b[35m:f32[]\u001b[39m = reduce_sum[axes=(0,)] b\n",
       "    d\u001b[35m:f32[]\u001b[39m = sqrt c\n",
       "  \u001b[34m\u001b[22m\u001b[1min \u001b[39m\u001b[22m\u001b[22m(d,) }"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Another example of a simple graph \n",
    "\n",
    "global_list = []\n",
    "\n",
    "#invalid function with side-effect \n",
    "def norm(x):\n",
    "    global_list.append(x)\n",
    "    x = x ** 2\n",
    "    n = x.sum()\n",
    "    n = jnp.sqrt(n)\n",
    "    return n \n",
    "\n",
    "jax.make_jaxpr(norm)(inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Automatic Differentiation\n",
    "\n",
    "Instead of backpropagating gradients through tensors, JAX takes as input a function, and outputs another function which directly calculates the gradients for it. It is done by ``jax.grad``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients:  [1.3333334 2.        2.6666667]\n"
     ]
    }
   ],
   "source": [
    "grad_function = jax.grad(simple_graph)\n",
    "gradients = grad_function(inp)\n",
    "print(\"Gradients: \", gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ \u001b[34m\u001b[22m\u001b[1mlambda \u001b[39m\u001b[22m\u001b[22m; a\u001b[35m:f32[3]\u001b[39m. \u001b[34m\u001b[22m\u001b[1mlet\n",
       "    \u001b[39m\u001b[22m\u001b[22mb\u001b[35m:f32[3]\u001b[39m = add a 2.0\n",
       "    c\u001b[35m:f32[3]\u001b[39m = integer_pow[y=2] b\n",
       "    d\u001b[35m:f32[3]\u001b[39m = integer_pow[y=1] b\n",
       "    e\u001b[35m:f32[3]\u001b[39m = mul 2.0 d\n",
       "    f\u001b[35m:f32[3]\u001b[39m = add c 3.0\n",
       "    g\u001b[35m:f32[]\u001b[39m = reduce_sum[axes=(0,)] f\n",
       "    _\u001b[35m:f32[]\u001b[39m = div g 3.0\n",
       "    h\u001b[35m:f32[]\u001b[39m = div 1.0 3.0\n",
       "    i\u001b[35m:f32[3]\u001b[39m = broadcast_in_dim[broadcast_dimensions=() shape=(3,)] h\n",
       "    j\u001b[35m:f32[3]\u001b[39m = mul i e\n",
       "  \u001b[34m\u001b[22m\u001b[1min \u001b[39m\u001b[22m\u001b[22m(j,) }"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get jaxpr representation of the gradient function\n",
    "jax.make_jaxpr(grad_function)(inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often, we dont only want the gradients, but also the actual output of the function, for  instance for logging the loss. It is done by ``jax.value_and_grad``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array(12.666667, dtype=float32),\n",
       " Array([1.3333334, 2.       , 2.6666667], dtype=float32))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_grad_function = jax.value_and_grad(simple_graph)\n",
    "val_grad_function(inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Speeding Up Computation with JUST-IN-TIME\n",
    "\n",
    "JAX takes full advantage of the available accelerator hardware, by compiling functions just-in-time with XLA (Accelerated Linear Algebra), using their JAXPR representation.\n",
    "\n",
    "It is done by ``jax.jit`` which can be either applied directly on function or used as the decorator before a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "jitted_function = jax.jit(simple_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new random subkey for generatting new random values \n",
    "\n",
    "rng, normal_rng = jax.random.split(rng)\n",
    "large_input = jax.random.normal(normal_rng, (1000,))\n",
    "#run the jitted function once to start compilation\n",
    "_ = jitted_function(large_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65.2 µs ± 1.2 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "simple_graph(large_input).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.35 µs ± 40.7 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "jitted_function(large_input).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also , we can apply multiple transformations on the same function in JAX such as applying ``jax.jit`` on a gradient function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "jitted_grad_function = jax.jit(grad_function)\n",
    "_ = jitted_grad_function(large_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.95 ms ± 54.1 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "grad_function(large_input).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.87 µs ± 89.4 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "jitted_grad_function(large_input).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
