{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax \n",
    "import jax.numpy as jnp \n",
    "from jax import grad, jit, vmap \n",
    "from jax import random\n",
    "\n",
    "key = random.key(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Higher-Order Derivatives \n",
    "\n",
    "JAX provides two transformation for computing the Jacobian of a function ``jax.jacfwd()`` and ``jax.jacrev()`` corresponding to forward and reverse mode auto-differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[2., 0., 0.],\n",
       "       [0., 2., 0.],\n",
       "       [0., 0., 2.]], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def hessian(f):\n",
    "    return jax.jacfwd(jax.grad(f))\n",
    "\n",
    "def f(x):\n",
    "    return jnp.dot(x, x)\n",
    "hessian(f)(jnp.array([1., 2., 3.]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To differentiate through gradient updates for __Model Agnostic Meta Learning (MAML)__, again JAX can be easily used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meta_loss_fn(params, data):\n",
    "    \"\"\"compute the loss after one step of SGD\"\"\"\n",
    "\n",
    "    grads = jax.grad(loss_fn)(params, data)\n",
    "    return loss_fn(params - lr * grads, data)\n",
    "\n",
    "meta_grads = jax.grad(meta_loss_fn)(params, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stopping Gradients\n",
    "\n",
    "Sometimes, you might want to avoid backpropagating gradients through some subset of the computational graph.\n",
    "\n",
    "In the case of __Temporal Difference TD(0)__ update, it is used to learn to estimate the value of a state in an environment from experience of interacting with the environment.\n",
    "\n",
    "TD(0) update to the network parameter is:\n",
    "$$Δθ = (r_{t} + v_{θ}(s_{t}) - v_{θ}(S_{t}-1)) ∇ v_{θ}(S_{t}-1)$$\n",
    "\n",
    "This update is not the gradient of any loss functino. But it can be written as the gradient of the psuedo-loss function\n",
    "\n",
    "$$L_{\\theta} = -\\frac{1}{2}[r_{t} + v_{θ}(s_{t})- v_{θ}(s_t-1)^2]$$\n",
    "\n",
    "if the __dependency of the target $r_t + v_{θ}(s_t)$ on the parameter $\\theta$ is ignored__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use ``jax.lax.stop_gradient()`` to force JAX to ignore the dependency of the target on $\\theta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Value function and initial parameters\n",
    "value_fn = lambda theta, state: jnp.dot(theta, state)\n",
    "theta = jnp.array([0.1, -0.1, 0.])\n",
    "\n",
    "#An example transition \n",
    "s_tm1 = jnp.array([1., 2., -1.])\n",
    "r_t = jnp.array(1.)\n",
    "s_t = jnp.array([2., 1., 0.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([ 1.2,  2.4, -1.2], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def td_loss(theta, s_tm1, r_t, s_t):\n",
    "\n",
    "    v_tm1 = value_fn(theta, s_tm1)\n",
    "    target = r_t + value_fn(theta, s_t)\n",
    "    return -0.5 * ((jax.lax.stop_gradient(target) - v_tm1) ** 2)\n",
    "\n",
    "td_update = jax.grad(td_loss)\n",
    "delta_theta = td_update(theta, s_tm1, r_t, s_t)\n",
    "delta_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([ 1.2,  2.4, -1.2], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cross-checking\n",
    "s_grad = jax.grad(value_fn)(theta, s_tm1)\n",
    "delta_theta = (r_t + value_fn(theta, s_t) - value_fn(theta, s_tm1)) * s_grad\n",
    "delta_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Straight-through estimator using ``stop_gradient``\n",
    "\n",
    "It is a trick for defining a _gradient_ of a function that is otherwise non-differentiable.\n",
    "\n",
    "Given a non-differentiable function, $f: \\mathbb{R}^n → \\mathbb{R}^n$ that is used as part of a larger function that we wish to find a gradient of, we simply pretend during the forward pass that $f$ is the identity function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(x): 3.0\n",
      "straight_through_f(3.2): 3.0\n",
      "grad(f)(x): 0.0\n",
      "grad(straight_through_f)(x): 1.0\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return jnp.round(x)\n",
    "\n",
    "def straight_through_f(x):\n",
    "    #create an exactly-zero expression with Sternbenz lemma that \n",
    "    #an exactly-one gradient\n",
    "\n",
    "    zero = x - jax.lax.stop_gradient(x)\n",
    "    return zero + jax.lax.stop_gradient(f(x))\n",
    "\n",
    "print(\"f(x):\", f(3.2))\n",
    "print(\"straight_through_f(3.2):\", straight_through_f(3.2))\n",
    "\n",
    "print(\"grad(f)(x):\", jax.grad(f)(3.2))\n",
    "print(\"grad(straight_through_f)(x):\", jax.grad(straight_through_f)(3.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Hessian-vector__ products with ``jax.grad`` of ``jax.grad``\n",
    "\n",
    "A Hessian-vector product function can be useful for minimizing smooth convex functions, or for studying the curvature of neural network training objectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hvp(f, x, v):\n",
    "    return grad(lambda x: jnp.vdot(grad(f)(x), v))(x) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jacobian and Hessian using ``jax.jacfwd`` and ``jax.jacrev``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jacfwd result, with shape (4, 3)\n",
      "[[ 0.10874642  0.23422305  0.16102834]\n",
      " [ 0.08166757 -0.10022839  0.01392061]\n",
      " [ 0.08774893  0.01012488 -0.21937233]\n",
      " [ 0.04643593 -0.15625063  0.08722425]]\n",
      "Jacrev result, with shape (4, 3)\n",
      "[[ 0.10874641  0.23422305  0.16102834]\n",
      " [ 0.08166758 -0.1002284   0.01392061]\n",
      " [ 0.08774893  0.01012488 -0.21937232]\n",
      " [ 0.04643593 -0.15625063  0.08722425]]\n"
     ]
    }
   ],
   "source": [
    "from jax import jacfwd, jacrev \n",
    "\n",
    "#define a sigmoid function \n",
    "\n",
    "def sigmoid(x):\n",
    "    return 0.5 * (jnp.tanh(x / 2.) + 1)\n",
    "\n",
    "#outputs probability of a label being true\n",
    "def predict(W, b, inputs):\n",
    "    return sigmoid(jnp.dot(inputs, W)+b)\n",
    "\n",
    "#build a toy dataset   \n",
    "\n",
    "inputs = jnp.array([[0.52, 1.12,  0.77],\n",
    "                     [0.88, -1.08, 0.15],\n",
    "                     [0.52, 0.06, -1.30],\n",
    "                     [0.74, -2.49, 1.39]])\n",
    "\n",
    "#initialize random model parameters\n",
    "key, W_key, b_key = random.split(key, 3)\n",
    "W = random.normal(W_key, (3,))\n",
    "b = random.normal(b_key, ())\n",
    "\n",
    "#isolate the function from th weights matrix to the predictions\n",
    "f = lambda W: predict(W, b, inputs)\n",
    "\n",
    "J = jacfwd(f)(W)\n",
    "print(\"Jacfwd result, with shape\", J.shape)\n",
    "print(J)\n",
    "\n",
    "J = jacrev(f)(W)\n",
    "print(\"Jacrev result, with shape\", J.shape)\n",
    "print(J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both functions of ``jax.jacfwd()`` and ``jax.jacrev()`` computes the same value.\n",
    "\n",
    "However,\n",
    "\n",
    "- ``jax.jacfwd()`` is more efficient for _tall_ Jacobian i.e, more outputs than inputs\n",
    "- ``jax.jacrev()`` is more for _wide_ Jacobian i.e, more inputs than outputs\n",
    "\n",
    "For matrices that are near-square, ``jax.jacfwd()`` has probably edge over other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jacobian form W to logits is\n",
      "[[ 0.10874641  0.23422305  0.16102834]\n",
      " [ 0.08166758 -0.1002284   0.01392061]\n",
      " [ 0.08774893  0.01012488 -0.21937232]\n",
      " [ 0.04643593 -0.15625063  0.08722425]]\n",
      "Jacobian form b to logits is\n",
      "[0.20912772 0.09280407 0.16874795 0.06275126]\n"
     ]
    }
   ],
   "source": [
    "#Both functions can be used with CONTAINER types\n",
    "\n",
    "def predict_dic(params, inputs):\n",
    "    return predict(params['W'], params['b'], inputs)\n",
    "\n",
    "J_dict = jacrev(predict_dic)({'W': W, 'b': b}, inputs)\n",
    "for k, v in J_dict.items():\n",
    "    print(\"Jacobian form {} to logits is\".format(k))\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a composition of two of these functions gives us a way to compute dense HESSIAN matrices\n",
    "\n",
    "To implement, ``hessian`` either way ``jacfwd(jacrev(f))`` or ``jacrev(jacfwd(f))`` would work, but __forward-over-reverse__ is typicall the most efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hessian with Shape (4, 3, 3)\n",
      "[[[-0.02286455 -0.04924672 -0.03385712]\n",
      "  [-0.04924672 -0.10606987 -0.07292303]\n",
      "  [-0.03385712 -0.07292303 -0.05013458]]\n",
      "\n",
      " [[ 0.05698795 -0.06993975  0.00971385]\n",
      "  [-0.06993975  0.08583514 -0.01192155]\n",
      "  [ 0.00971385 -0.01192155  0.00165577]]\n",
      "\n",
      " [[ 0.02601311  0.00300151 -0.06503277]\n",
      "  [ 0.00300151  0.00034633 -0.00750378]\n",
      "  [-0.06503277 -0.00750378  0.16258194]]\n",
      "\n",
      " [[ 0.02973893 -0.10006747  0.05586096]\n",
      "  [-0.10006747  0.33671352 -0.18796457]\n",
      "  [ 0.05586096 -0.18796457  0.10492801]]]\n"
     ]
    }
   ],
   "source": [
    "def hessian(f):\n",
    "    return jacfwd(jacrev(f))\n",
    "\n",
    "H = hessian(f)(W)\n",
    "print(\"Hessian with Shape\", H.shape)\n",
    "print(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
